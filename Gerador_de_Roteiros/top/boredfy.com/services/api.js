/**
 * api.js
 * * This module centralizes all interactions with external APIs by
 * calling our own backend server.
 */

import { state } from "./state.js";

// Apontar para o backend local da pr√≥pria aplica√ß√£o
export const BACKEND_URL = "/api";

// Fun√ß√£o auxiliar para obter headers de autentica√ß√£o com sess√£o
export async function getAuthHeaders() {
  if (window.fingerprintManager && window.fingerprintManager.getAuthHeaders) {
    return await window.fingerprintManager.getAuthHeaders();
  } else {
    // Fallback para m√©todo antigo se fingerprintManager n√£o estiver dispon√≠vel
    const user = firebase.auth().currentUser;
    if (!user) throw new Error("Usu√°rio n√£o autenticado");

    const token = await user.getIdToken();
    return {
      "Content-Type": "application/json",
      Authorization: `Bearer ${token}`,
    };
  }
}

export async function callGenerativeAI(
  apiKey,
  userPrompt,
  signal,
  isPremise = false,
  isBlockOfScript = false
) {
  const headers = await getAuthHeaders();

  const response = await fetch(`${BACKEND_URL}/generate-text`, {
    method: "POST",
    headers,
    body: JSON.stringify({
      apiKey: apiKey,
      prompt: userPrompt,
      model: state.selectedGeminiModel,
      isPremise: isPremise,
      isBlockOfScript: isBlockOfScript, // Nova flag para blocos individuais
    }),
    signal: signal,
  });

  const data = await response.json();

  if (!response.ok || !data.success) {
    throw new Error(
      data.message || "Erro desconhecido ao gerar texto no servidor."
    );
  }

  return data.text;
}

export async function trackCompleteScript(scriptData) {
  const headers = await getAuthHeaders();

  const response = await fetch(`${BACKEND_URL}/track-complete-script`, {
    method: "POST",
    headers,
    body: JSON.stringify(scriptData),
  });

  const data = await response.json();

  if (!response.ok || !data.success) {
    console.warn("Falha ao trackear roteiro completo:", data.message);
    // N√£o interrompe o fluxo se tracking falhar
  }

  return data;
}

export async function trackCompleteTTS(ttsData) {
  const headers = await getAuthHeaders();

  const response = await fetch(`${BACKEND_URL}/track-complete-tts`, {
    method: "POST",
    headers,
    body: JSON.stringify(ttsData),
  });

  const data = await response.json();

  if (!response.ok || !data.success) {
    console.warn("Falha ao trackear TTS completo:", data.message);
    // N√£o interrompe o fluxo se tracking falhar
  }

  return data;
}

export async function generateTTS(
  textChunks,
  languageCode,
  voiceId,
  ttsApiKey
) {
  // console.log(`Enviando requisi√ß√£o de TTS para o backend: ${languageCode}`);

  const headers = await getAuthHeaders();

  try {
    const response = await fetch(`${BACKEND_URL}/tts`, {
      method: "POST",
      headers,
      body: JSON.stringify({ textChunks, languageCode, voiceId, ttsApiKey }),
      // Aumentar timeout para dar mais tempo ao servidor
      signal: AbortSignal.timeout(180000), // 3 minutos
    });

    // Verificar content-type antes de ler o body
    const contentType = response.headers.get("content-type");
    // console.log(
    //   `Resposta TTS - Status: ${response.status}, Content-Type: ${contentType}`
    // );

    // Ler o response apenas UMA vez
    const responseText = await response.text();

    if (!response.ok) {
      // Tentar fazer parse do JSON se poss√≠vel
      if (contentType && contentType.includes("application/json")) {
        try {
          const error = JSON.parse(responseText);
          return {
            success: false,
            message: error.message || "Falha na gera√ß√£o de TTS no servidor.",
          };
        } catch (parseError) {
          console.error("Erro ao fazer parse da resposta de erro:", parseError);
        }
      }

      console.error(
        "Resposta de erro do servidor:",
        responseText.substring(0, 500)
      );
      return {
        success: false,
        message: `Erro do servidor (${
          response.status
        }): ${responseText.substring(0, 100)}...`,
      };
    }

    // Verificar se √© JSON antes de fazer parse
    if (!contentType || !contentType.includes("application/json")) {
      console.error("Resposta n√£o √© JSON:", responseText.substring(0, 500));
      return {
        success: false,
        message: "Servidor retornou resposta inv√°lida (n√£o JSON)",
      };
    }

    const data = JSON.parse(responseText);
    // console.log(`TTS conclu√≠do - Sucesso: ${data.success}`);
    return data;
  } catch (error) {
    console.error("Erro na requisi√ß√£o TTS:", error);

    if (error.name === "TimeoutError") {
      return {
        success: false,
        message:
          "Timeout na gera√ß√£o de √°udio - servidor demorou muito para responder",
      };
    }

    return { success: false, message: `Erro de comunica√ß√£o: ${error.message}` };
  }
}

export async function generateTTSChunkByChunk(
  textChunks,
  languageCode,
  voiceId,
  ttsApiKeys,
  resultContainer,
  addResultLog
) {
  // console.log(
  //   `üéµ Iniciando gera√ß√£o de a√∫dio: ${textChunks.length} peda√ßos para ${languageCode}`
  // );
  // console.log(
  //   `üîë Usando ${ttsApiKeys.length} API keys - processando ${Math.min(
  //     ttsApiKeys.length,
  //     textChunks.length
  //   )} peda√ßos por vez`
  // );

  const headers = await getAuthHeaders();
  const audioChunks = new Array(textChunks.length); // Array com tamanho fixo para manter ordem
  const allJobIds = []; // Para capturar TODOS os jobIds dos chunks

  // Processar chunks em grupos baseado no n√∫mero de API keys
  const batchSize = ttsApiKeys.length;

  for (
    let batchStart = 0;
    batchStart < textChunks.length;
    batchStart += batchSize
  ) {
    const batchEnd = Math.min(batchStart + batchSize, textChunks.length);
    const currentBatch = [];

    const batchNumber = Math.floor(batchStart / batchSize) + 1;
    const totalBatches = Math.ceil(textChunks.length / batchSize);

    // console.log(
    //   `üì¶ Processando lote ${batchNumber}: peda√ßos ${
    //     batchStart + 1
    //   }-${batchEnd}/${textChunks.length}`
    // );
    addResultLog(
      resultContainer,
      `üì¶ Processando lote ${batchNumber}/${totalBatches}: peda√ßos ${
        batchStart + 1
      }-${batchEnd}/${textChunks.length}`
    );

    // Criar promises para o lote atual
    for (let i = batchStart; i < batchEnd; i++) {
      const chunkIndex = i;
      const textChunk = textChunks[chunkIndex];
      const apiKeyIndex = chunkIndex % ttsApiKeys.length; // Round-robin das API keys
      const ttsApiKey = ttsApiKeys[apiKeyIndex];

      // console.log(
      //   `üîÑ Preparando peda√ßo ${chunkIndex + 1}/${
      //     textChunks.length
      //   } com API key ${ttsApiKey.substring(0, 10)}...`
      // );
      addResultLog(
        resultContainer,
        `üîÑ Preparando peda√ßo ${chunkIndex + 1}/${
          textChunks.length
        } com API key ${ttsApiKey.substring(0, 10)}...`
      );

      const chunkPromise = processChunk(
        textChunk,
        chunkIndex,
        textChunks.length,
        languageCode,
        voiceId,
        ttsApiKey,
        headers
      );
      currentBatch.push({ promise: chunkPromise, index: chunkIndex });
    }

    // Processar lote atual em paralelo
    try {
      const batchResults = await Promise.all(
        currentBatch.map((item) => item.promise)
      );

      // Armazenar resultados na posi√ß√£o correta e capturar TODOS os jobIds
      batchResults.forEach((result, idx) => {
        const chunkIndex = currentBatch[idx].index;
        audioChunks[chunkIndex] = result.audio_base_64; // Extrair apenas o √°udio

        // Capturar TODOS os jobIds (cada chunk tem seu pr√≥prio jobId)
        if (result.jobId) {
          allJobIds.push(result.jobId);
        }

        // console.log(
        //   `‚úÖ Peda√ßo ${chunkIndex + 1}/${
        //     textChunks.length
        //   } processado com sucesso!`
        // );
        addResultLog(
          resultContainer,
          `‚úÖ Peda√ßo ${chunkIndex + 1}/${
            textChunks.length
          } processado com sucesso!`
        );
      });

      // Delay entre lotes (exceto no √∫ltimo)
      if (batchEnd < textChunks.length) {
        // console.log(`‚è≥ Aguardando 3 segundos antes do pr√≥ximo lote...`);
        addResultLog(
          resultContainer,
          `‚è≥ Aguardando 3 segundos antes do pr√≥ximo lote...`
        );
        await new Promise((resolve) => setTimeout(resolve, 3000));
      }
    } catch (error) {
      console.error(`‚ùå Erro no lote ${batchNumber}:`, error);
      addResultLog(
        resultContainer,
        `‚ùå Erro no lote ${batchNumber}: ${error.message}`,
        "error"
      );
      throw new Error(
        `Falha no lote de peda√ßos ${batchStart + 1}-${batchEnd}: ${
          error.message
        }`
      );
    }
  }

  // Verificar se todos os chunks foram processados
  const missingChunks = audioChunks
    .map((chunk, idx) => (chunk ? null : idx))
    .filter((idx) => idx !== null);
  if (missingChunks.length > 0) {
    const errorMsg = `Peda√ßos faltando: ${missingChunks
      .map((idx) => idx + 1)
      .join(", ")}`;
    addResultLog(resultContainer, `‚ùå ${errorMsg}`, "error");
    throw new Error(errorMsg);
  }

  // Combinar todos os chunks de √°udio
  // console.log(`üîó Combinando ${audioChunks.length} peda√ßos de √°udio...`);
  addResultLog(
    resultContainer,
    `üîó Combinando ${audioChunks.length} peda√ßos de √°udio...`
  );

  const combinedAudio = combineAudioChunks(audioChunks);

  // console.log(
  //   `‚úÖ TTS paralelo conclu√≠do com sucesso! (${ttsApiKeys.length} APIs simult√¢neas)`
  // );
  addResultLog(
    resultContainer,
    `üîó TTS paralelo conclu√≠do com sucesso! Salvando no servidor...`
  );

  // NOVA FUNCIONALIDADE: Salvar √°udio no servidor ao inv√©s de retornar base64
  try {
    const saveResponse = await fetch(`${BACKEND_URL}/save-generated-content`, {
      method: 'POST',
      headers: await getAuthHeaders(),
      body: JSON.stringify({
        type: 'audio',
        content: combinedAudio,
        metadata: {
          language: languageCode,
          voice: voiceId,
          chunks: audioChunks.length,
          apis_used: ttsApiKeys.length
        }
      })
    });

    if (!saveResponse.ok) {
      throw new Error('Falha ao salvar √°udio no servidor');
    }

    const saveData = await saveResponse.json();
    
    if (!saveData.success) {
      throw new Error(saveData.message || 'Erro ao salvar √°udio');
    }

    addResultLog(
      resultContainer,
      `üíæ √Åudio salvo no servidor! (${(saveData.size/1024/1024).toFixed(1)}MB)`
    );

    return {
      success: true,
      serverPath: saveData.serverPath, // NOVO: Caminho no servidor
      fileId: saveData.fileId,
      chunks_processed: audioChunks.length,
      apis_used: ttsApiKeys.length,
      parallel_processing: true,
      jobIds: allJobIds, // Array com TODOS os jobIds para tracking posterior
      size: saveData.size,
      // N√ÉO retornar mais audio_base_64 para economizar mem√≥ria!
    };

  } catch (saveError) {
    console.error('‚ùå Erro cr√≠tico ao salvar √°udio no servidor:', saveError);
    addResultLog(
      resultContainer,
      `‚ùå ERRO: Falha ao salvar √°udio no servidor: ${saveError.message}`,
      'error'
    );

    // SEM FALLBACK - sistema novo deve funcionar sempre
    throw new Error(`Falha cr√≠tica ao salvar √°udio no servidor: ${saveError.message}`);
  }
}

// Fun√ß√£o auxiliar para processar um chunk individual
async function processChunk(
  textChunk,
  chunkIndex,
  totalChunks,
  languageCode,
  voiceId,
  ttsApiKey,
  headers
) {
  try {
    const response = await fetch(`${BACKEND_URL}/tts`, {
      method: "POST",
      headers,
      body: JSON.stringify({
        textChunk,
        chunkIndex,
        totalChunks,
        languageCode,
        voiceId,
        ttsApiKey,
      }),
      // Timeout por chunk individual
      signal: AbortSignal.timeout(90000), // 90 segundos por chunk
    });

    // Verificar content-type antes de ler o body
    const contentType = response.headers.get("content-type");

    // Ler o response apenas UMA vez
    const responseText = await response.text();

    if (!response.ok) {
      // Tentar fazer parse do JSON se poss√≠vel
      if (contentType && contentType.includes("application/json")) {
        try {
          const error = JSON.parse(responseText);
          throw new Error(
            error.message || "Falha na gera√ß√£o de TTS no servidor."
          );
        } catch (parseError) {
          console.error("Erro ao fazer parse da resposta de erro:", parseError);
        }
      }

      console.error(
        `Erro no chunk ${chunkIndex + 1}:`,
        responseText.substring(0, 500)
      );
      throw new Error(
        `Erro do servidor no chunk ${chunkIndex + 1} (${
          response.status
        }): ${responseText.substring(0, 100)}...`
      );
    }

    // Verificar se √© JSON antes de fazer parse
    if (!contentType || !contentType.includes("application/json")) {
      console.error(
        `Chunk ${chunkIndex + 1} - Resposta n√£o √© JSON:`,
        responseText.substring(0, 500)
      );
      throw new Error(
        `Servidor retornou resposta inv√°lida para chunk ${
          chunkIndex + 1
        } (n√£o JSON)`
      );
    }

    const data = JSON.parse(responseText);

    if (!data.success) {
      throw new Error(data.message || `Falha no chunk ${chunkIndex + 1}`);
    }

    return {
      audio_base_64: data.audio_base_64,
      jobId: data.jobId, // Capturar jobId do response
    };
  } catch (error) {
    console.error(`‚ùå Erro no chunk ${chunkIndex + 1}:`, error);

    if (error.name === "TimeoutError") {
      throw new Error(
        `Timeout no peda√ßo ${
          chunkIndex + 1
        } - servidor demorou muito para responder`
      );
    }

    throw new Error(`Falha no peda√ßo ${chunkIndex + 1}: ${error.message}`);
  }
}

// Fun√ß√£o para combinar chunks de √°udio base64
function combineAudioChunks(audioChunks) {
  // console.log(`üîó Combinando ${audioChunks.length} peda√ßos de √°udio...`);

  // Converter cada chunk base64 para bytes e combinar
  const audioBuffers = audioChunks.map((chunk) => {
    // Decodificar base64 para bytes
    const binaryString = atob(chunk);
    const bytes = new Uint8Array(binaryString.length);
    for (let i = 0; i < binaryString.length; i++) {
      bytes[i] = binaryString.charCodeAt(i);
    }
    return bytes;
  });

  // Calcular tamanho total
  const totalLength = audioBuffers.reduce(
    (sum, buffer) => sum + buffer.length,
    0
  );

  // Criar buffer combinado
  const combinedBuffer = new Uint8Array(totalLength);
  let offset = 0;

  for (const buffer of audioBuffers) {
    combinedBuffer.set(buffer, offset);
    offset += buffer.length;
  }

  // Converter de volta para base64
  let binaryString = "";
  for (let i = 0; i < combinedBuffer.length; i++) {
    binaryString += String.fromCharCode(combinedBuffer[i]);
  }

  const combinedBase64 = btoa(binaryString);
  // console.log(`‚úÖ √Åudio combinado: ${combinedBase64.length} caracteres`);

  return combinedBase64;
}

export async function fetchUrlContent(urlToFetch) {
  // console.log(
  //   `Enviando requisi√ß√£o de fetch de URL para o backend: ${urlToFetch}`
  // );

  const headers = await getAuthHeaders();
  delete headers["Content-Type"]; // Remove Content-Type para GET request

  const response = await fetch(
    `${BACKEND_URL}/fetch-url?url=${encodeURIComponent(urlToFetch)}`,
    {
      headers,
    }
  );

  if (!response.ok) {
    const error = await response.json();
    return {
      success: false,
      error: error.error || "Falha ao buscar conte√∫do da URL no servidor.",
    };
  }

  return await response.json();
}

// Fun√ß√£o principal para criar ZIP - NOVA IMPLEMENTA√á√ÉO OTIMIZADA
export async function createZipFromInlineFiles(files) {
  console.log('üîÑ [Compatibility] createZipFromInlineFiles redirecionando para sistema otimizado');
  return await createOptimizedZip(files);
}

// IMPLEMENTA√á√ÉO LEGACY (desabilitada) - mantida para refer√™ncia  
async function createZipFromInlineFilesLegacy(files) {
  const headers = await getAuthHeaders();

  // Verificar tamanho total antes de enviar
  const totalSize = files.reduce((sum, f) => {
    const contentSize = f?.content?.length || 0;
    return sum + contentSize;
  }, 0);

  console.log(
    `üìä Tamanho total dos arquivos: ${(totalSize / 1024 / 1024).toFixed(2)} MB`
  );

  // Limite de seguran√ßa: 50MB para UPLOAD (proxy reverso/nginx limita)
  const MAX_UPLOAD_SIZE = 50 * 1024 * 1024; // 50MB em bytes

  if (totalSize > MAX_UPLOAD_SIZE) {
    // Se for muito grande para UPLOAD, dividir em chunks, mas o servidor criar√° 1 ZIP √∫nico
    console.log(
      `‚ö†Ô∏è Arquivos muito grandes para upload (${(
        totalSize /
        1024 /
        1024
      ).toFixed(2)} MB), enviando em partes... Servidor criar√° 1 ZIP √∫nico.`
    );
    return await uploadInChunksAndCreateSingleZip(files, totalSize);
  }

  // SEMPRE usar inlineFiles - o backend salva no servidor primeiro
  const response = await fetch(`${BACKEND_URL}/create-zip-from-server-files`, {
    method: "POST",
    headers,
    body: JSON.stringify({ inlineFiles: files }),
  });

  // Verifica content-type antes de tentar fazer .json()
  const contentType = response.headers.get("content-type");
  if (!contentType || !contentType.includes("application/json")) {
    const textResponse = await response.text();
    throw new Error(
      `Resposta n√£o JSON (poss√≠vel HTML/Cloudflare): ${textResponse.substring(
        0,
        100
      )}...`
    );
  }

  const data = await response.json();
  if (!response.ok || !data.success) {
    throw new Error(data.message || "Erro ao criar o zip no backend.");
  }
  return data.downloadUrl;
}

// Fun√ß√£o para enviar arquivos grandes em chunks e criar 1 ZIP √∫nico no servidor
async function uploadInChunksAndCreateSingleZip(files, totalSize) {
  const CHUNK_SIZE = 40 * 1024 * 1024; // 40MB por chunk (seguro para proxies)
  const chunks = [];
  let currentChunk = [];
  let currentChunkSize = 0;

  console.log(
    `üì¶ Dividindo ${files.length} arquivos em chunks de ${(
      CHUNK_SIZE /
      1024 /
      1024
    ).toFixed(1)}MB para upload...`
  );

  // Dividir arquivos em chunks
  for (const file of files) {
    const fileSize = file?.content?.length || 0;

    // Se adicionar este arquivo ultrapassar o limite do chunk atual
    if (currentChunkSize + fileSize > CHUNK_SIZE && currentChunk.length > 0) {
      chunks.push([...currentChunk]);
      currentChunk = [];
      currentChunkSize = 0;
    }

    currentChunk.push(file);
    currentChunkSize += fileSize;
  }

  // Adicionar √∫ltimo chunk se n√£o estiver vazio
  if (currentChunk.length > 0) {
    chunks.push(currentChunk);
  }

  console.log(`üì¶ Criados ${chunks.length} chunks para upload`);

  // Criar um sessionId √∫nico para esta opera√ß√£o
  const sessionId = `session_${Date.now()}_${Math.random()
    .toString(36)
    .slice(2, 8)}`;
  console.log(`üîó Sess√£o criada: ${sessionId}`);

  // Enviar cada chunk para o servidor (que vai salv√°-los)
  for (let i = 0; i < chunks.length; i++) {
    const chunk = chunks[i];
    const chunkSize = chunk.reduce(
      (sum, f) => sum + (f?.content?.length || 0),
      0
    );

    console.log(
      `üì¶ Enviando chunk ${i + 1}/${chunks.length} (${(
        chunkSize /
        1024 /
        1024
      ).toFixed(2)} MB)...`
    );

    try {
      const headers = await getAuthHeaders();
      const response = await fetch(`${BACKEND_URL}/upload-chunk-for-zip`, {
        method: "POST",
        headers,
        body: JSON.stringify({
          sessionId,
          chunkIndex: i,
          totalChunks: chunks.length,
          files: chunk,
          isLastChunk: i === chunks.length - 1,
        }),
      });

      // Verifica content-type antes de tentar fazer .json()
      const contentType = response.headers.get("content-type");
      if (!contentType || !contentType.includes("application/json")) {
        const textResponse = await response.text();
        throw new Error(
          `Resposta n√£o JSON no chunk ${i + 1}: ${textResponse.substring(
            0,
            100
          )}...`
        );
      }

      const data = await response.json();
      if (!response.ok || !data.success) {
        throw new Error(data.message || `Erro ao enviar chunk ${i + 1}`);
      }

      console.log(`‚úÖ Chunk ${i + 1}/${chunks.length} enviado com sucesso`);

      // Se √© o √∫ltimo chunk, o servidor criou o ZIP √∫nico
      if (i === chunks.length - 1 && data.zipCreated) {
        console.log(
          `üéâ ZIP √∫nico criado com sucesso! ${data.totalFiles} arquivos`
        );
        return data.downloadUrl; // Retorna URL do ZIP √∫nico
      }
    } catch (error) {
      console.error(`‚ùå Erro no chunk ${i + 1}:`, error);
      throw error;
    }
  }

  throw new Error(
    "Erro inesperado: ZIP n√£o foi criado ap√≥s enviar todos os chunks"
  );
}

// ============= NOVO SISTEMA ZIP OTIMIZADO =============

/**
 * Fun√ß√£o otimizada para cria√ß√£o de ZIP usando streaming
 * Substitui sistema fragmentado por solu√ß√£o unificada
 */
/**
 * NOVA FUN√á√ÉO: createOptimizedZip usando STREAMING PURO
 * Esta √© a solu√ß√£o DEFINITIVA que contorna o limite de 100MB do Cloudflare
 */
export async function createOptimizedZip(files, options = {}) {
  // Valida√ß√£o b√°sica
  if (!files || !Array.isArray(files) || files.length === 0) {
    throw new Error('Nenhum arquivo fornecido para cria√ß√£o do ZIP');
  }

  const startTime = Date.now();

  // Preparar arquivos para streaming (apenas refer√™ncias e pequenos arquivos)
  const streamFiles = files.map(file => {
    // Priorizar serverPath (arquivos j√° no servidor)
    if (file.serverPath) {
      return {
        name: file.name,
        serverPath: file.serverPath
        // N√ÉO enviar content para economizar bandwidth
      };
    } 
    
    // Para arquivos pequenos sem serverPath, manter inline
    if (file.content && file.content.length < 1024 * 1024) { // < 1MB
      return {
        name: file.name,
        content: file.content
      };
    }

    // Arquivos grandes sem serverPath - problema!
    console.warn(`‚ö†Ô∏è Arquivo grande sem serverPath: ${file.name} (${(file.content?.length || 0 / 1024).toFixed(1)}KB)`);
    return {
      name: file.name,
      content: file.content || '[Conte√∫do n√£o dispon√≠vel]'
    };
  });

  // Calcular payload (deve ser muito pequeno agora)
  const payloadSize = JSON.stringify(streamFiles).length;
  console.log(`üöÄ [StreamZIP] Iniciando download streaming: ${files.length} arquivos, payload: ${(payloadSize/1024).toFixed(1)}KB`);

  try {
    // Usar novo endpoint de streaming
    const response = await fetch(`${BACKEND_URL}/download-zip-stream`, {
      method: "POST",
      headers: await getAuthHeaders(),
      body: JSON.stringify({ 
        files: streamFiles,
        name: options.name || null
      }),
    });

    if (!response.ok) {
      // Para streaming, erro pode vir como JSON ou text
      const contentType = response.headers.get("content-type");
      if (contentType && contentType.includes("application/json")) {
        const errorData = await response.json();
        throw new Error(errorData.message || "Erro no servidor de streaming");
      } else {
        const errorText = await response.text();
        throw new Error(`Erro do servidor: ${errorText.substring(0, 100)}...`);
      }
    }

    // Para streaming, response √© o arquivo ZIP diretamente
    const blob = await response.blob();
    const processingTime = Date.now() - startTime;
    
    console.log(`‚úÖ [StreamZIP] Download conclu√≠do em ${processingTime}ms (${(blob.size/1024/1024).toFixed(2)}MB)`);
    
    // Retornar blob para download direto
    return blob;

  } catch (error) {
    const processingTime = Date.now() - startTime;
    console.error(`‚ùå [StreamZIP] Erro ap√≥s ${processingTime}ms:`, error.message);
    throw new Error(`Erro no streaming de ZIP: ${error.message}`);
  }
}

// Fun√ß√£o legada (mantida para compatibilidade) - agora usa sistema otimizado
export async function createZipNoBackend(files) {
  console.log('‚ö†Ô∏è [Compatibility] Usando createZipNoBackend legado - redirecionando para sistema otimizado');
  return await createOptimizedZip(files);
}

// Lista arquivos do usu√°rio autenticado (zips salvos)
export async function listUserFiles() {
  const headers = await getAuthHeaders();
  delete headers["Content-Type"]; // GET
  const response = await fetch(`${BACKEND_URL}/user-files`, { headers });
  const data = await response.json();
  if (!response.ok || !data.success) {
    throw new Error(data.message || "Erro ao listar arquivos");
  }
  return data.files || [];
}

// Adiciona arquivos aos "Meus Arquivos" automaticamente (sem download imediato)
export async function addToMyFiles(files, name = null) {
  // console.log(`üîÑ addToMyFiles iniciado com ${files.length} arquivos`);
  
  // SIMPLES: Enviar OS MESMOS arquivos que o download usa!
  const headers = await getAuthHeaders();
  
  const response = await fetch(`${BACKEND_URL}/add-user-file`, {
    method: "POST",
    headers,
    body: JSON.stringify({
      files, // OS MESMOS ARQUIVOS (com serverPath quando dispon√≠vel)
      name,
    }),
  });

  // Verifica content-type antes de tentar fazer .json()
  const contentType = response.headers.get("content-type");
  if (!contentType || !contentType.includes("application/json")) {
    const textResponse = await response.text();
    throw new Error(
      `Resposta n√£o JSON (poss√≠vel HTML/Cloudflare): ${textResponse.substring(
        0,
        100
      )}...`
    );
  }

  const data = await response.json();
  if (!response.ok || !data.success) {
    throw new Error(data.message || "Erro ao adicionar arquivos");
  }

  // console.log(`‚úÖ ZIP salvo em 'Meus Arquivos': ${data.name}`);
  return data;
}


// Download autenticado de um arquivo do usu√°rio
export async function downloadUserFile(downloadUrl) {
  const headers = await getAuthHeaders();
  delete headers["Content-Type"]; // GET
  const response = await fetch(downloadUrl, { headers });
  if (!response.ok) {
    const text = await response.text();
    throw new Error(
      `Falha no download (${response.status}): ${text.substring(0, 120)}`
    );
  }
  const blob = await response.blob();
  // Tenta extrair nome do arquivo do header
  const cd = response.headers.get("content-disposition") || "";
  let filename = "arquivo.zip";
  const match = /filename\*=UTF-8''([^;]+)|filename="?([^";]+)"?/i.exec(cd);
  if (match) {
    filename = decodeURIComponent(match[1] || match[2] || filename);
  }
  return { blob, filename };
}

// Fun√ß√£o para salvar agentes personalizados
export async function saveCustomAgents(customAgents) {
  const headers = await getAuthHeaders();

  const response = await fetch(`${BACKEND_URL}/save-agents`, {
    method: "POST",
    headers,
    body: JSON.stringify({ customAgents }),
  });

  const data = await response.json();
  if (!response.ok || !data.success) {
    throw new Error(data.message || "Erro ao salvar agentes");
  }
  return data;
}

// Fun√ß√£o para carregar agentes personalizados
export async function loadCustomAgents() {
  const headers = await getAuthHeaders();
  delete headers["Content-Type"]; // Remove Content-Type para GET request

  const response = await fetch(`${BACKEND_URL}/get-agents`, {
    headers,
  });

  const data = await response.json();
  if (!response.ok || !data.success) {
    throw new Error(data.message || "Erro ao carregar agentes");
  }
  return data.customAgents || {};
}

// Fun√ß√£o para validar API Keys
export async function validateApiKey(apiKey, apiType) {
  const headers = await getAuthHeaders();

  try {
    const requestBody = { apiKey, apiType };

    // Incluir modelo apenas para valida√ß√£o do Gemini
    if (apiType === "gemini") {
      requestBody.model = state.selectedGeminiModel;
    }

    const response = await fetch(`${BACKEND_URL}/validate-api-key`, {
      method: "POST",
      headers,
      body: JSON.stringify(requestBody),
    });

    const data = await response.json();

    if (!response.ok) {
      return {
        success: false,
        validation: {
          valid: false,
          message: data.message || "Erro na valida√ß√£o",
          details: { statusCode: response.status },
        },
      };
    }

    return data;
  } catch (error) {
    return {
      success: false,
      validation: {
        valid: false,
        message: "Erro de comunica√ß√£o com o servidor",
        details: { error: error.message },
      },
    };
  }
}
